# Introduction

In this set of experimets, the idea is to use a seq2seq machine translation
model for attmepting to generate less ambiguous sentences from more ambiguous 
ones.

# Observations Log

## Including POS Tags (SpaCy)

- NLL loss increases until about 50,000 cases. It decreases 100,000 onwards

- 2 hour increase in training time for the entire dataset, have optimized the code slightly

- Treat POS Tags as possible vocabulary items (so are indexed as well)

- Qualitative evaluation reveals minor improvements in the ability to predict function word insertions, so the hypothesis worked.

## Including Dependency Information (SpaCy)

- Could not finish experiments on this for two reasons: a) I found a better representation b) Any inclusion of dependency information is a graph convolution problem for sparse graphs. 

- Representation: 

	nsubj	dobj	...	det

The	0	0	...	1

cat	2	0	...	0

ate	0	0	...	0	<- ROOT

the	0	0	...	4

mouse	0	2	...	0

This is a _very_ sparse matrix with $n$ non-zero elements where $n$ is the number of words. 

- Convolution on sparse matrices: Apparently this is not as simple as it sounds. After padding this dependency matrix into a square matrix, we could either perform sparse convolution, or make the matrix denser before performing. I do not know the theoretical differences between the two, except that standard convolutional filters provide very limited information from sparse matrices. Working on making the matrices dense.

## Experiments with BERT

- Code has been written, but it is throwing some very novel CuDA errors which I have never seen before, including a device shift for some reason, which I do not understand. Debugging that now.

## Irshad's Classification Task

Irshad used DyNET for his work, which has been giving me installation errors. I should have a better hang of the last two topics by next week.
