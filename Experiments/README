# Introduction

In this set of experimets, the idea is to use a seq2seq machine translation
model for attmepting to generate less ambiguous sentences from more ambiguous 
ones.

# Observations Log

## BERT Model

After further tweaking with the model and running it for 1M samples, BERT tends
to outperform word embeddings used before. One idea would be augment the BERT
model with POS tag, dependency parse and other information.

## Intermediate Prediction Layer

The prediction of change based on 0 and 1 for unchanged and changed positions
was not a success and has been entirely scrapped until further notice. I believe
that the dataset should be much more tuned for this to work.

## Classification Results and Analysis

Both 0s: 609563 cases
Characteristic cases include:
  - Anaphora resolution
  - Synonyms
  - Misspellings

Both 1s: 1510130 cases
Characteristic cases include:
  - Insertions
  - Contractions and Expansions
  - Paraphrases

Clearly: this classification helps in denoising, but it is not enough. We also
need to look into how to use context information in order to determine which
edits actually disambiguate the sentence at hand.

## Tasks for next week

Extract more information from the scraped data such as section name, context
sentences, and identify differences in the following criteria:
  - changes in semantic roles of nouns in sentences with similar constructions
  - changes in closed category POS tags and associated dependencies (including
    CC, DT, EX, PDT, POS, PRP, RP, WDT, WP($), UH, TO, WRB).

Also aim to filter instructions based on section, so sections such as
``Introduction``, ``Related wikis`` etc are going to be removed after
preprocessing.
